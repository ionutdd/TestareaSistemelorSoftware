
## Introduction
Code coverage is a vital metric in software testing, reflecting the extent to which the source code is exercised by the test suite. However, including autogenerated code in coverage reports can distort this metric, often presenting a lower coverage percentage than what accurately represents the manually written code. This case study examines how excluding autogenerated code from coverage analysis significantly improved the reported code coverage for the "FMInatorul" project, a web application built with ASP.NET Core, without altering the codebase itself.

## Background
The FMInatorul project leverages ASP.NET Core with Entity Framework Core for database operations and Razor pages for its user interface. Initial coverage analysis revealed a line coverage of just **2.6%** and a branch coverage of **6.9%**, figures that were misleading due to the inclusion of extensive autogenerated code—such as Entity Framework migration files and Razor page compilations—which typically lack direct test coverage.

## Approach
To obtain a more accurate representation of test coverage for the manually written code, we adjusted the coverage analysis process by excluding autogenerated files. This was achieved using specific filters in the ReportGenerator tool. The commands used for generating the reports were:

- **Initial Report (Bad Report):**
  ```
  reportgenerator -reports:"FMInatorul.Tests/TestResults/**/coverage.cobertura.xml" -targetdir:"coverage-report2" -reporttypes:Html
  ```
  This command included all files, resulting in a skewed coverage metric.

- **Improved Report (Good Report):**
  ```
  reportgenerator -reports:"FMInatorul.Tests/TestResults/*/coverage.cobertura.xml" -targetdir:"coverage-report" -reporttypes:Html -assemblyfilters:+FMInatorul -classfilters:"-FMInatorul.Migrations.;-FMInatorul.Areas.Identity.;-AspNetCoreGeneratedDocument.;-AutoGeneratedProgram;-SeedData"
  ```
  Here, we applied filters to include only the `FMInatorul` assembly and excluded classes related to migrations, identity areas, Razor page compilations, autogenerated programs, and seed data.

## Results
The exclusion of autogenerated code led to a substantial improvement in the coverage metrics:

### Initial Report (Bad Report)
- **Covered Lines:** 308
- **Uncovered Lines:** 11,277
- **Coverable Lines:** 11,585
- **Total Lines:** 16,257
- **Line Coverage:** 2.6% (308 / 11,585)
- **Covered Branches:** 47
- **Total Branches:** 676
- **Branch Coverage:** 6.9% (47 / 676)

### Improved Report (Good Report)
- **Covered Lines:** 308
- **Uncovered Lines:** 852
- **Coverable Lines:** 1,160
- **Total Lines:** 2,156
- **Line Coverage:** 26.5% (308 / 1,160)
- **Covered Branches:** 47
- **Total Branches:** 324
- **Branch Coverage:** 14.5% (47 / 324)

Notably, the number of covered lines remained constant at **308**, indicating no new tests were added. However, the coverable lines dropped dramatically from **11,585** to **1,160**, reflecting the exclusion of autogenerated code. Similarly, total branches decreased from **676** to **324**, boosting the branch coverage percentage.

### Risk Hotspots and CRAP Score
The CRAP (Change Risk Anti-Patterns) score, which combines cyclomatic complexity and code coverage to assess change risk, also shifted focus. In the initial report, high CRAP scores (e.g., **1980** for `<ExecuteAsync()` in `Views_Shared__Layout` with a cyclomatic complexity of **44**) were often tied to autogenerated code. In the improved report, these were excluded, and the hotspots highlighted manually written code, such as:
- `SubmitAnswer()` in `RoomHub`: CRAP score **420**, cyclomatic complexity **20**
- `CreateRoom()` in `RoomsController`: CRAP score **210**, cyclomatic complexity **14**

This shift allowed developers to prioritize testing efforts on critical, custom code rather than framework-generated artifacts.

## Discussion
Excluding autogenerated code from coverage analysis offers several benefits:

1. **Accuracy:** The revised metrics (26.5% line coverage and 14.5% branch coverage) more accurately reflect the test coverage of code that developers are responsible for testing.
2. **Focus:** It directs attention to improving test coverage for business logic and custom implementations, rather than untestable autogenerated code.
3. **Motivation:** A higher coverage percentage better represents the team’s testing efforts, enhancing morale and clarity.

However, the unchanged number of covered lines (**308**) underscores that actual test coverage of the manually written code still requires improvement. The reduction in CRAP scores for excluded autogenerated code does not imply a decrease in risk across the board but rather a reorientation toward meaningful risk hotspots. Cyclomatic complexity remained tied to specific methods (e.g., **20** for `SubmitAnswer()`), indicating areas needing simplification or better testing, not an overall increase across the project.

## Conclusion
By excluding autogenerated code from the coverage analysis, the FMInatorul project’s reported code coverage improved from **2.6% to 26.5%** for lines and **6.9% to 14.5%** for branches, without modifying the codebase. This adjustment provided a clearer, more actionable view of test coverage, refining the focus on manually written code and reducing the noise from high CRAP scores in autogenerated sections. For projects using frameworks that generate substantial code, this approach is recommended to ensure coverage metrics reflect true testing efficacy.

## Recommendations
- **Refine Filters:** Periodically review and adjust filters to maintain relevance as the project evolves.
- **Enhance Testing:** Use the improved metrics to target uncovered lines (e.g., the **852** remaining) and branches in custom code.
- **CI/CD Integration:** Incorporate this filtered coverage analysis into the continuous integration pipeline to track progress over time.
